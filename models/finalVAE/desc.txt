HYPERPARAMS
16 latent dims
64 hidden nodes
Conv model

TRAINING STATS
1 epoch of pretraining
64 epochs of fine tuning
Roughly 2.5 + 1 hours on laptop
0.04 loss give or take
regularization off

OTHER NOTES
new KL divergence formulation
gradual introduction of KL divergence term via exponential function starting from lr * 1e-6 and ending at lr

OUTPUT
similar to the kind of output during the semester after pretraining
Couldn't see results after fine tuning, IDE was hanging 
